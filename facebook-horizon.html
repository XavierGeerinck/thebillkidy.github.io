<!DOCTYPE html>
<html>
<head>
    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Facebook's Open-Source Reinforcement Learning Platform - A Deep Dive</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <!-- highlight.js -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/vs2015.min.css">
    <style>.hljs { background: none; }</style>

    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="My thoughts, tutorials and learnings" />
    <link rel="shortcut icon" href="/assets/images/favicon.png" type="image/png" />
    <link rel="canonical" href="/facebook-horizon" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Xavier Geerinck - Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Facebook's Open-Source Reinforcement Learning Platform - A Deep Dive" />
    <meta property="og:description" content="Facebook decided to open-source the platform that they created to solve end-to-end Reinforcement Learning problems at the scale they are working on. So of course I just had to try this ;) Let’s go through this together on how they installed it and what you should do to get this" />
    <meta property="og:url" content="/facebook-horizon" />
    <meta property="og:image" content="/assets/images/covers/horizon.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2018-12-02T09:00:00+00:00" />
    <meta property="article:modified_time" content="2018-12-02T09:00:00+00:00" />
    <meta property="article:tag" content="Ai" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Facebook's Open-Source Reinforcement Learning Platform - A Deep Dive" />
    <meta name="twitter:description" content="Facebook decided to open-source the platform that they created to solve end-to-end Reinforcement Learning problems at the scale they are working on. So of course I just had to try this ;) Let’s go through this together on how they installed it and what you should do to get this" />
    <meta name="twitter:url" content="/" />
    <meta name="twitter:image" content="/assets/images/covers/horizon.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Xavier Geerinck - Blog" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="Ai" />
    <meta name="twitter:site" content="@XavierGeerinck" />
    <meta name="twitter:creator" content="@XavierGeerinck" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Xavier Geerinck - Blog",
        "logo": "/false"
    },
    "url": "/facebook-horizon",
    "image": {
        "@type": "ImageObject",
        "url": "/assets/images/covers/horizon.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/facebook-horizon"
    },
    "description": "Facebook decided to open-source the platform that they created to solve end-to-end Reinforcement Learning problems at the scale they are working on. So of course I just had to try this ;) Let’s go through this together on how they installed it and what you should do to get this"
}
    </script>

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Facebook's Open-Source Reinforcement Learning Platform - A Deep Dive" href="/feed.xml" />



    <!-- Enable MathJAX for LaTeX equations -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });
        </script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-about" role="menuitem"><a href="/projects">Projects</a></li>
    <li class="nav-about" role="menuitem"><a href="/demos">Demos</a></li>
    <li class="nav-about" role="menuitem"><a href="/categories">Categories</a></li>
</ul>

    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
                <a class="social-link social-link-tw" href="https://twitter.com/XavierGeerinck" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            
            
                <a class="social-link social-link-tw" href="https://linkedin.com/in/xaviergeerinck" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg></a>
            
        </div>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full post ">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime=" 2 December 2018"> 2 December 2018</time>
                    
                        <span class="date-divider">/</span>
                        
                            
                               <a href='/tag/ai/'>AI</a>,
                            
                        
                            
                               <a href='/tag/ai-ml/'>AI-ML</a>,
                            
                        
                            
                               <a href='/tag/ai-rl/'>AI-RL</a>
                            
                        
                    
                </section>
            </header>

            
            <figure class="post-full-image" style="background-image: url(/assets/images/covers/horizon.png)">
            </figure>
            

            <section class="post-full-content">
                <div class="kg-card-markdown">
                        <h1 class="post-full-title">Facebook's Open-Source Reinforcement Learning Platform - A Deep Dive</h1>
                    <p>Facebook decided to open-source the platform that they created to solve end-to-end Reinforcement Learning problems at the scale they are working on. So of course I just had to try this ;) Let’s go through this together on how they installed it and what you should do to get this working yourself.</p>

<blockquote>
  <p>I started by creating a brand new installation of Ubuntu 18.10
Also tested and verified working on Windows Subsystem for Linux</p>
</blockquote>

<h2 id="installing-anaconda">Installing Anaconda</h2>

<p>Let’s start by installing Anaconda, this is easily done by navigating to the documentation located at <a href="https://conda.io/docs/user-guide/install/index.html">https://conda.io/docs/user-guide/install/index.html</a> whereafter we can find the link to the Linux Installer <a href="https://www.anaconda.com/download/#linux">https://www.anaconda.com/download/#linux</a> which will give us the installer script: <a href="https://repo.anaconda.com/archive/Anaconda3-5.3.0-Linux-x86_64.sh">https://repo.anaconda.com/archive/Anaconda3-5.3.0-Linux-x86_64.sh</a>.</p>

<p>We can download and run this by running:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl https://repo.anaconda.com/archive/Anaconda3-5.3.0-Linux-x86_64.sh <span class="nt">-o</span> conda.sh
bash conda.sh
</code></pre></div></div>

<p>Then follow the steps of the installer to install Anaconda</p>

<p>Once you did this, add the conda installation to your <code class="highlighter-rouge">PATH</code> variable through:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s1">'export PATH="$PATH":/home/&lt;YOUR_USER&gt;/anaconda3/bin'</span> <span class="o">&gt;&gt;</span> ~/.bashrc
<span class="nb">.</span> ~/.bashrc
</code></pre></div></div>

<blockquote>
  <p>Note: the default installation of anaconda is /home/ubuntu/anaconda3 for the Python 3 version</p>
</blockquote>

<h2 id="configuring-anaconda-for-our-horizon-installation">Configuring Anaconda for our Horizon Installation</h2>

<p>Facebook Horizon requires several channels for certain software which we can easily add to anaconda:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda config <span class="nt">--add</span> channels conda-forge <span class="c"># ONNX/tensorboardX</span>
conda config <span class="nt">--add</span> channels pytorch 
</code></pre></div></div>

<h2 id="installing-horizon">Installing Horizon</h2>

<blockquote>
  <p>For a deeper dive on how you can install Horizon, check out the Git repo: <a href="https://github.com/facebookresearch/Horizon/blob/master/docs/installation.md">https://github.com/facebookresearch/Horizon/blob/master/docs/installation.md</a></p>
</blockquote>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/facebookresearch/Horizon.git
<span class="nb">cd </span>Horizon/
conda <span class="nb">install</span> <span class="sb">`</span><span class="nb">cat </span>docker/requirements.txt<span class="sb">`</span> <span class="c"># wait till it solved the environment, then select y</span>
<span class="nb">source </span>activate base <span class="c"># Activate the base conda environment</span>

pip <span class="nb">install </span>onnx <span class="c"># Install ONNX</span>
<span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span><span class="nb">dirname</span> <span class="si">$(</span><span class="nb">dirname</span> <span class="nt">--</span> <span class="sb">`</span>which conda<span class="sb">`</span><span class="si">))</span><span class="s2">"</span> <span class="c"># Set JAVA_HOME to anaconda installation</span>
<span class="nb">cd</span> <span class="c"># Go back to root</span>

<span class="c"># Install Spark 2.3.1</span>
wget http://www-eu.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz
<span class="nb">tar</span> <span class="nt">-xzf</span> spark-2.3.1-bin-hadoop2.7.tgz
<span class="nb">sudo mv </span>spark-2.3.1-bin-hadoop2.7 /usr/local/spark

<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:/usr/local/spark/bin <span class="c"># add to PATH so we can find spark-submit</span>

<span class="c"># Install OpenAI Gym</span>
pip <span class="nb">install</span> <span class="s2">"gym[classic_control,box2d,atari]"</span>

<span class="c"># Build Thrift Classes</span>
<span class="nb">cd </span>Horizon/
thrift <span class="nt">--gen</span> py <span class="nt">--out</span> <span class="nb">.</span> ml/rl/thrift/core.thrift

<span class="c"># Build Horizon</span>
pip <span class="nb">install</span> <span class="nt">-e</span> <span class="nb">.</span> <span class="c"># we use "-e" for "ephemral package" which will instantly reflect changes in the package</span>
</code></pre></div></div>

<h2 id="horizon-global-overview">Horizon (Global Overview)</h2>

<h3 id="introduction">Introduction</h3>

<p>Horizon is an End-To-End platform which <em>“includes workflows for simulated environments as well as a distributed platform for preprocessing, training, and exporting models in production.”</em> - <a href="https://code.fb.com/ml-applications/horizon/">(Source)</a></p>

<p>From reading the <a href="https://research.fb.com/wp-content/uploads/2018/10/Horizon-Facebooks-Open-Source-Applied-Reinforcement-Learning-Platform.pdf">paper</a> we can read that this platform was created with the following in mind:</p>

<ul>
  <li>Ability to Handle Large Datasets Efficiently</li>
  <li>Ability to Preprocess Data Automatically &amp; Efficiently</li>
  <li>Competitive Algorithmic Performance</li>
  <li>Algorithm Performance Estimates before Launch</li>
  <li>Flexible Model Serving in Production</li>
  <li>Platform Reliability</li>
</ul>

<p>Which sounds awesome to me, so let’s get started by how we are able to utilize this platform, whereafter we can do a more deep-dive in how it works.</p>

<blockquote>
  <p>For some of the terminology used in Reinforcement Learning, feel free to check my <a href="/rl-overview-terminology">previous blog post</a> about it.</p>
</blockquote>

<h3 id="getting-started">Getting Started</h3>

<p>Getting started with Horizon is as easy as checking the <a href="https://github.com/facebookresearch/Horizon/blob/master/docs/usage.md">usage</a> documentation written by them. This includes the steps</p>

<ol>
  <li>Creating training data</li>
  <li>Converting the data to the timeline format</li>
  <li>Create the normalization parameters</li>
  <li>Train the model</li>
  <li>Evaluate the model</li>
</ol>

<h2 id="horizon---batch-rl-deep-dive">Horizon - Batch RL (Deep-Dive)</h2>

<p>Now we know what the general idea of the Horizon platform is, let’s run through the different steps as written in the usage document and deep-dive on them, discovering what is going on behind the scenes. Let’s start by creating our training data.</p>

<h3 id="1-creating-training-data">1. Creating Training Data</h3>

<p>Our usage document lists that we should create the training data through these commands:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create a directory where we will put the training data</span>
<span class="nb">mkdir </span>cartpole_discrete

<span class="c"># Generate training data</span>
python ml/rl/test/gym/run_gym.py <span class="nt">-p</span> ml/rl/test/gym/discrete_dqn_cartpole_v0_100_eps.json <span class="nt">-f</span> cartpole_discrete/training_data.json
</code></pre></div></div>

<p>but what does this actually do? Opening up the <code class="highlighter-rouge">run_gym.py</code> file located in <code class="highlighter-rouge">ml/rl/test/gym/</code> shows us the following in the <code class="highlighter-rouge">main()</code> method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="s">"Train a RL net to play in an OpenAI Gym environment."</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"-p"</span><span class="p">,</span> <span class="s">"--parameters"</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Path to JSON parameters file."</span><span class="p">)</span>
</code></pre></div></div>

<p>Which shows us that when running the command <code class="highlighter-rouge">python ml/rl/test/gym/run_gym.py</code> that we are able to see the usage of our script in the console, running this results in:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Traceback <span class="o">(</span>most recent call last<span class="o">)</span>:
  File <span class="s2">"ml/rl/test/gym/run_gym.py"</span>, line 611, <span class="k">in</span> &lt;module&gt;
    + <span class="s2">" [-s &lt;score_bar&gt;] [-g &lt;gpu_id&gt;] [-l &lt;log_level&gt;] [-f &lt;filename&gt;]"</span>
Exception: Usage: python run_gym.py <span class="nt">-p</span> &lt;parameters_file&gt; <span class="o">[</span><span class="nt">-s</span> &lt;score_bar&gt;] <span class="o">[</span><span class="nt">-g</span> &lt;gpu_id&gt;] <span class="o">[</span><span class="nt">-l</span> &lt;log_level&gt;] <span class="o">[</span><span class="nt">-f</span> &lt;filename&gt;]
</code></pre></div></div>

<p>Explaining us that if we give the parameter file defined by our <code class="highlighter-rouge">-p</code> parameter it will load this JSON file and load it into a variable called <code class="highlighter-rouge">params</code>, while if we add the <code class="highlighter-rouge">-f</code> parameter, we will be able to save the collected samples as an RLDataSet to the provided file.</p>

<h4 id="main-method">main() Method</h4>

<p>The main method will now continue doing a couple of things:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load our parameters from the json
</span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="s">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="c1"># Initialize a dataset variable of type `RLDataset` if the `file_path` parameter is set
#    `file_path`: If set, save all collected samples as an RLDataset to this file.
</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">RLDataset</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">file_path</span><span class="p">)</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">file_path</span> <span class="k">else</span> <span class="bp">None</span>

<span class="c1"># Call the method `run_gym` with the parameters and arguments provided
</span><span class="n">reward_history</span><span class="p">,</span> <span class="n">timestep_history</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">predictor</span> <span class="o">=</span> <span class="n">run_gym</span><span class="p">(</span>
    <span class="n">params</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">score_bar</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">gpu_id</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">start_saving_from_episode</span>
<span class="p">)</span>

<span class="c1"># Save our dataset if provided through the -f parameter
</span><span class="k">if</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="c1">#  Save the results to a csv if the `results_file_path` parameter is set
</span><span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">results_file_path</span><span class="p">:</span>
    <span class="n">write_lists_to_csv</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">results_file_path</span><span class="p">,</span> <span class="n">reward_history</span><span class="p">,</span> <span class="n">timestep_history</span><span class="p">)</span>

<span class="c1"># Return our reward history
</span><span class="k">return</span> <span class="n">reward_history</span>
</code></pre></div></div>

<p>After running our command shown in the usage document <code class="highlighter-rouge">python ml/rl/test/gym/run_gym.py -p ml/rl/test/gym/discrete_dqn_cartpole_v0_100_eps.json -f cartpole_discrete/training_data.json</code> we can see the following structure in the <code class="highlighter-rouge">training_data.json</code> file which was defined by the <code class="highlighter-rouge">-f</code> parameter.</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"ds"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2019-01-01"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"mdp_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"0"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"sequence_number"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w">
    </span><span class="nl">"state_features"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"0"</span><span class="p">:</span><span class="w"> </span><span class="mf">-0.032091656679586175</span><span class="p">,</span><span class="w">
        </span><span class="nl">"1"</span><span class="p">:</span><span class="w"> </span><span class="mf">-0.016310561477682117</span><span class="p">,</span><span class="w">
        </span><span class="nl">"2"</span><span class="p">:</span><span class="w"> </span><span class="mf">-0.01312794549150956</span><span class="p">,</span><span class="w">
        </span><span class="nl">"3"</span><span class="p">:</span><span class="w"> </span><span class="mf">-0.04438365281404494</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="nl">"action"</span><span class="p">:</span><span class="w"> </span><span class="s2">"1"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"reward"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w">
    </span><span class="nl">"action_probability"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w">
    </span><span class="nl">"possible_actions"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="s2">"0"</span><span class="p">,</span><span class="w">
        </span><span class="s2">"1"</span><span class="w">
    </span><span class="p">],</span><span class="w">
    </span><span class="nl">"metrics"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"reward"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<h4 id="rldataset-class">RLDataset Class</h4>

<p>This is generated by the <code class="highlighter-rouge">-f</code> parameter that will be saving the results in a format provided by the <code class="highlighter-rouge">RLDataset</code> class to the file provided. Checking this class located at <code class="highlighter-rouge">ml/rl/training/rl_dataset.py</code> shows us:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
Holds a collection of RL samples in the "pre-timeline" format.

:param file_path: String Load/save the dataset from/to this file.
"""</span>

<span class="c1"># === LINES REMOVED ===
</span>
<span class="bp">self</span><span class="o">.</span><span class="n">rows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
<span class="p">{</span>
    <span class="s">"ds"</span><span class="p">:</span> <span class="s">"2019-01-01"</span><span class="p">,</span>  <span class="c1"># Fix ds for simplicity in open source examples
</span>    <span class="s">"mdp_id"</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">mdp_id</span><span class="p">),</span>
    <span class="s">"sequence_number"</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">sequence_number</span><span class="p">),</span>
    <span class="s">"state_features"</span><span class="p">:</span> <span class="n">state_features</span><span class="p">,</span>
    <span class="s">"action"</span><span class="p">:</span> <span class="n">action</span><span class="p">,</span>
    <span class="s">"reward"</span><span class="p">:</span> <span class="n">reward</span><span class="p">,</span>
    <span class="s">"action_probability"</span><span class="p">:</span> <span class="n">action_probability</span><span class="p">,</span>
    <span class="s">"possible_actions"</span><span class="p">:</span> <span class="n">possible_actions</span><span class="p">,</span>
    <span class="s">"metrics"</span><span class="p">:</span> <span class="p">{</span><span class="s">"reward"</span><span class="p">:</span> <span class="n">reward</span><span class="p">},</span>
<span class="p">}</span>
</code></pre></div></div>

<h4 id="run_gym-method">run_gym() Method</h4>

<p>Now we can see that the lines are being created and saved in-memory in the <code class="highlighter-rouge">RLDataset</code> class. But what is actually using this and filling it in? Let’s first take a look at our <code class="highlighter-rouge">run_gym()</code> method in general:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env_type</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s">"env"</span><span class="p">]</span>

<span class="c1"># Initialize the OpenAI Gym Environment
</span><span class="n">env</span> <span class="o">=</span> <span class="n">OpenAIGymEnvironment</span><span class="p">(</span>
    <span class="n">env_type</span><span class="p">,</span>
    <span class="n">rl_parameters</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span>
    <span class="n">rl_parameters</span><span class="o">.</span><span class="n">softmax_policy</span><span class="p">,</span>
    <span class="n">rl_parameters</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">OpenAIGymMemoryPool</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s">"max_replay_memory_size"</span><span class="p">])</span>
<span class="n">model_type</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s">"model_type"</span><span class="p">]</span>

<span class="n">use_gpu</span> <span class="o">=</span> <span class="n">gpu_id</span> <span class="o">!=</span> <span class="n">USE_CPU</span>

<span class="c1"># Use the "training" {} parameters and "model_type": "&lt;MODEL&gt;" model_type
# to create a trainer as the ones listed in /ml/rl/training/*_trainer.py
# The model_type is defined in /ml/rl/test/gym/open_ai_gym_environment.py
</span><span class="n">trainer</span> <span class="o">=</span> <span class="n">create_trainer</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s">"model_type"</span><span class="p">],</span> <span class="n">params</span><span class="p">,</span> <span class="n">rl_parameters</span><span class="p">,</span> <span class="n">use_gpu</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>

<span class="c1"># Create a GymDQNPredictor based on the ModelType and Trainer above
# This is located in /ml/rl/test/gym/gym_predictor.py
</span><span class="n">predictor</span> <span class="o">=</span> <span class="n">create_predictor</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">model_type</span><span class="p">,</span> <span class="n">use_gpu</span><span class="p">)</span>

<span class="n">c2_device</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">DeviceOption</span><span class="p">(</span>
    <span class="n">caffe2_pb2</span><span class="o">.</span><span class="n">CUDA</span> <span class="k">if</span> <span class="n">use_gpu</span> <span class="k">else</span> <span class="n">caffe2_pb2</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">gpu_id</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Train using SGD (stochastic gradient descent)
# This just passess the parameters given towards a method called train_gym_online_rl which will train our algorithm
</span><span class="k">return</span> <span class="n">train_sgd</span><span class="p">(</span>
    <span class="n">c2_device</span><span class="p">,</span>
    <span class="n">env</span><span class="p">,</span>
    <span class="n">replay_buffer</span><span class="p">,</span>
    <span class="n">model_type</span><span class="p">,</span>
    <span class="n">trainer</span><span class="p">,</span>
    <span class="n">predictor</span><span class="p">,</span>
    <span class="s">"{} test run"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">env_type</span><span class="p">),</span>
    <span class="n">score_bar</span><span class="p">,</span>
    <span class="o">**</span><span class="n">params</span><span class="p">[</span><span class="s">"run_details"</span><span class="p">],</span>
    <span class="n">save_timesteps_to_dataset</span><span class="o">=</span><span class="n">save_timesteps_to_dataset</span><span class="p">,</span>
    <span class="n">start_saving_from_episode</span><span class="o">=</span><span class="n">start_saving_from_episode</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<p>The <code class="highlighter-rouge">run_gym</code> method appears to be using the parameters that we loaded from our JSON file to initialize the OpenAI Gym Environment. So let’s see how one of these JSON files look like by opening one, running a quick <code class="highlighter-rouge">cat ml/rl/test/gym/discrete_dqn_cartpole_v0_100_eps.json</code> shows us:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"env"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CartPole-v0"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"model_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"pytorch_discrete_dqn"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"max_replay_memory_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">10000</span><span class="p">,</span><span class="w">
  </span><span class="nl">"use_gpu"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
  </span><span class="nl">"rl"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.99</span><span class="p">,</span><span class="w">
    </span><span class="nl">"target_update_rate"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.2</span><span class="p">,</span><span class="w">
    </span><span class="nl">"reward_burnin"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="nl">"maxq_learning"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="nl">"epsilon"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="nl">"temperature"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.35</span><span class="p">,</span><span class="w">
    </span><span class="nl">"softmax_policy"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"rainbow"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"double_q_learning"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
    </span><span class="nl">"dueling_architecture"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"training"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
      </span><span class="mi">-1</span><span class="p">,</span><span class="w">
      </span><span class="mi">128</span><span class="p">,</span><span class="w">
      </span><span class="mi">64</span><span class="p">,</span><span class="w">
      </span><span class="mi">-1</span><span class="w">
    </span><span class="p">],</span><span class="w">
    </span><span class="nl">"activations"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
      </span><span class="s2">"relu"</span><span class="p">,</span><span class="w">
      </span><span class="s2">"relu"</span><span class="p">,</span><span class="w">
      </span><span class="s2">"linear"</span><span class="w">
    </span><span class="p">],</span><span class="w">
    </span><span class="nl">"minibatch_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w">
    </span><span class="nl">"learning_rate"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.001</span><span class="p">,</span><span class="w">
    </span><span class="nl">"optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ADAM"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"lr_decay"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.999</span><span class="p">,</span><span class="w">
    </span><span class="nl">"use_noisy_linear_layers"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"run_details"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"num_episodes"</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span><span class="p">,</span><span class="w">
    </span><span class="nl">"max_steps"</span><span class="p">:</span><span class="w"> </span><span class="mi">200</span><span class="p">,</span><span class="w">
    </span><span class="nl">"train_every_ts"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="nl">"train_after_ts"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="nl">"test_every_ts"</span><span class="p">:</span><span class="w"> </span><span class="mi">2000</span><span class="p">,</span><span class="w">
    </span><span class="nl">"test_after_ts"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="nl">"num_train_batches"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
    </span><span class="nl">"avg_over_num_episodes"</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Which shows that the <code class="highlighter-rouge">Environment</code>, <code class="highlighter-rouge">Epsilon</code>, <code class="highlighter-rouge">Softmax Policy</code> and <code class="highlighter-rouge">Gamma parameters</code> are all used to boot op the <code class="highlighter-rouge">OpenAIGymEnvironment</code> with the remaining parameters being passed to the trainer. Next to that the <code class="highlighter-rouge">run_gym</code> method will also initialize a replay_buffer, create the trainer and create predictor. Whereafter it will run the <code class="highlighter-rouge">train_sgd</code> method.</p>

<p>Since we now know our <code class="highlighter-rouge">run_gym()</code> method, let’s look further at how our <code class="highlighter-rouge">dataset</code> variable is passed further:</p>

<ul>
  <li><code class="highlighter-rouge">run_gym()</code> will get the method passed by the <code class="highlighter-rouge">main()</code> method as the <code class="highlighter-rouge">save_timesteps_to_dataset</code> parameter</li>
  <li><code class="highlighter-rouge">run_gym()</code> will pass this to the <code class="highlighter-rouge">train_sgd()</code> method</li>
  <li><code class="highlighter-rouge">train_sgd()</code> will pass it to the <code class="highlighter-rouge">train_gym_online_rl()</code> method.</li>
</ul>

<h4 id="train_gym_online_rl-method">train_gym_online_rl() Method</h4>

<p>When this parameter is now defined, the <code class="highlighter-rouge">train_gym_online_rl()</code> method will save several variables through the <code class="highlighter-rouge">insert()</code> method defined in the <code class="highlighter-rouge">RLDataset</code> class:</p>

<blockquote>
  <p>Remember that the <code class="highlighter-rouge">RLDataset</code> class was defined in the file: <code class="highlighter-rouge">ml/rl/training/rl_dataset.py</code> 
The <code class="highlighter-rouge">insert</code> method is defined as: <code class="highlighter-rouge">RLDataset::insert(mdp_id, sequence_number, state, action, reward, terminal, possible_actions, time_diff, action_probability)</code></p>
</blockquote>

<p>Source: <a href="https://github.com/facebookresearch/Horizon/blob/bbea36948bd409f03ec449be4539bd6bd9006418/ml/rl/test/gym/run_gym.py#L208">run_gym.py#L208</a></p>

<table>
  <thead>
    <tr>
      <th>Source Variable from <code class="highlighter-rouge">run_gym.py</code></th>
      <th>Output Variable</th>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>i</td>
      <td>mdp_id</td>
      <td><code class="highlighter-rouge">string</code></td>
      <td>A unique ID for the episode (e.g. an entire playthrough of a game)</td>
    </tr>
    <tr>
      <td>ep_timesteps - 1</td>
      <td>sequence_number</td>
      <td><code class="highlighter-rouge">integer</code></td>
      <td>Defines the ordering of states in an MDP (e.g. the timestamp of an event)</td>
    </tr>
    <tr>
      <td>state.tolist()</td>
      <td>state_features</td>
      <td><code class="highlighter-rouge">map&lt;integer, float&gt;</code></td>
      <td>A set of features describing the state.</td>
    </tr>
    <tr>
      <td>action_to_log</td>
      <td>action</td>
      <td><code class="highlighter-rouge">string</code></td>
      <td>The name of the action chosen</td>
    </tr>
    <tr>
      <td>reward</td>
      <td>reward</td>
      <td><code class="highlighter-rouge">float</code></td>
      <td>The reward at this state/action</td>
    </tr>
    <tr>
      <td>terminal</td>
      <td>terminal</td>
      <td><code class="highlighter-rouge">bool</code></td>
      <td>Not Used</td>
    </tr>
    <tr>
      <td>possible_actions</td>
      <td>possible_actions</td>
      <td><code class="highlighter-rouge">list&lt;string&gt;</code></td>
      <td>A list of all possible actions at this state. Note that the action taken must be present in this list.</td>
    </tr>
    <tr>
      <td>1.0</td>
      <td>action_probability</td>
      <td><code class="highlighter-rouge">float</code></td>
      <td>The probability of taking this action if the policy is stochastic, else null. Note that we strongly encourage using a stochastic policy instead of choosing the best action at every timestep. This exploration will improve the evaluation and ultimately result in better learned policies.</td>
    </tr>
    <tr>
      <td>1</td>
      <td>time_diff</td>
      <td><code class="highlighter-rouge">integer</code></td>
      <td>Not Ised</td>
    </tr>
    <tr>
      <td>?</td>
      <td>ds</td>
      <td><code class="highlighter-rouge">string</code></td>
      <td>A unique ID for this dataset</td>
    </tr>
  </tbody>
</table>

<p>Our <code class="highlighter-rouge">run_gym.py</code> will now run for a certain amount of episodes specified in the <code class="highlighter-rouge">-p</code> file (e.g. <code class="highlighter-rouge">ml/rl/test/gym/discrete_dqn_cartpole_v0_100_eps.json</code>) where it will (while it’s able to) use the <code class="highlighter-rouge">train_gym_online_rl()</code> method to:</p>
<ul>
  <li>Get the possible actions</li>
  <li>Take an action (based on if it’s a DISCRETE action_type or not)</li>
  <li>Step through the Gym Environment and retrieve the <code class="highlighter-rouge">next_state</code>, <code class="highlighter-rouge">reward</code> and <code class="highlighter-rouge">terminal</code> variables</li>
  <li>Define the next_action to take based on the <code class="highlighter-rouge">policy</code> in the <code class="highlighter-rouge">gym_env.policy</code> variable</li>
  <li>Increase the reward received</li>
  <li>Insert the observed behaviour in the replay buffer</li>
  <li>Every <code class="highlighter-rouge">train_every_ts</code> take <code class="highlighter-rouge">num_train_batches</code> from the <code class="highlighter-rouge">replay_buffer</code> and train the <code class="highlighter-rouge">trainer</code> with these
    <ul>
      <li>Note: this trainer is created in the <a href="https://github.com/facebookresearch/Horizon/blob/bbea36948bd409f03ec449be4539bd6bd9006418/ml/rl/test/gym/run_gym.py#L208"><code class="highlighter-rouge">create_trainer()</code></a> method which will create a DDPGTrainer, SACTrainer, ParametericDQNTrainer or DQNTrainer</li>
    </ul>
  </li>
  <li>Every <code class="highlighter-rouge">test_every_ts</code> log how our model is performing to the <code class="highlighter-rouge">logger</code>, <code class="highlighter-rouge">avg_reward_history</code> and <code class="highlighter-rouge">timestep_history</code></li>
  <li>Log when the episode ended</li>
</ul>

<h3 id="2-converting-our-training-data-to-the-timeline-format">2. Converting our Training data to the Timeline format</h3>

<p>In step 2 we will be converting our earlier training data that was saved in the format:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"ds"</span><span class="p">:</span><span class="w"> </span><span class="s2">"2019-01-01"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"mdp_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"0"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"sequence_number"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w">
    </span><span class="nl">"state_features"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"0"</span><span class="p">:</span><span class="w"> </span><span class="mf">-0.032091656679586175</span><span class="p">,</span><span class="w">
        </span><span class="nl">"1"</span><span class="p">:</span><span class="w"> </span><span class="mf">-0.016310561477682117</span><span class="p">,</span><span class="w">
        </span><span class="nl">"2"</span><span class="p">:</span><span class="w"> </span><span class="mf">-0.01312794549150956</span><span class="p">,</span><span class="w">
        </span><span class="nl">"3"</span><span class="p">:</span><span class="w"> </span><span class="mf">-0.04438365281404494</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="nl">"action"</span><span class="p">:</span><span class="w"> </span><span class="s2">"1"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"reward"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w">
    </span><span class="nl">"action_probability"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w">
    </span><span class="nl">"possible_actions"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="s2">"0"</span><span class="p">,</span><span class="w">
        </span><span class="s2">"1"</span><span class="w">
    </span><span class="p">],</span><span class="w">
    </span><span class="nl">"metrics"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"reward"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="w">
    </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Towards something they call the <code class="highlighter-rouge">timeline</code> format, this is a format that given a table (state, action, mdp_id, sequence_number, reward, possible_next_actions) returns the table needed for Reinforcement Learning (mdp_id, state_features, action, reward, next_state_features, next_action, sequence_number, sequence_number_ordinal, time_diff, possible_next_actions) defined in <a href="https://github.com/facebookresearch/Horizon/blob/master/preprocessing/src/main/scala/com/facebook/spark/rl/Timeline.scala">Timeline.scala</a> which we can represent as:</p>

<p><img src="/assets/images/posts/fb-horizon/timeline-format-map.png" alt="/assets/images/posts/fb-horizon/timeline-format-map.png" /></p>

<p>This will execute a Spark Job, run a query through Hive and return the results in a different file.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Build timeline package (only need to do this first time)</span>
mvn <span class="nt">-f</span> preprocessing/pom.xml clean package

<span class="c"># Clear last run's spark data (in case of interruption)</span>
<span class="nb">rm</span> <span class="nt">-Rf</span> spark-warehouse derby.log metastore_db preprocessing/spark-warehouse preprocessing/metastore_db preprocessing/derby.log

<span class="c"># Run timelime on pre-timeline data</span>
/usr/local/spark/bin/spark-submit <span class="se">\</span>
  <span class="nt">--class</span> com.facebook.spark.rl.Preprocessor preprocessing/target/rl-preprocessing-1.1.jar <span class="se">\</span>
  <span class="s2">"</span><span class="sb">`</span><span class="nb">cat </span>ml/rl/workflow/sample_configs/discrete_action/timeline.json<span class="sb">`</span><span class="s2">"</span>

<span class="c"># Merge output data to single file</span>
<span class="nb">mkdir </span>training_data
<span class="nb">mv </span>cartpole_discrete_timeline/part<span class="k">*</span> training_data/cartpole_training_data.json

<span class="c"># Remove the output data folder</span>
<span class="nb">rm</span> <span class="nt">-Rf</span> cartpole_discrete_timeline
</code></pre></div></div>

<p>After execution we can now watch the created file by running: <code class="highlighter-rouge">head -n1 training_data/cartpole_training_data.json</code>:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
	</span><span class="nl">"mdp_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"31"</span><span class="p">,</span><span class="w">
	</span><span class="nl">"sequence_number"</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w">
	</span><span class="nl">"propensity"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w">
	</span><span class="nl">"state_features"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nl">"0"</span><span class="p">:</span><span class="w"> </span><span class="mf">-0.029825548651835395</span><span class="p">,</span><span class="w">
		</span><span class="nl">"1"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.19730168855281788</span><span class="p">,</span><span class="w">
		</span><span class="nl">"2"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.013065490574540607</span><span class="p">,</span><span class="w">
		</span><span class="nl">"3"</span><span class="p">:</span><span class="w"> </span><span class="mf">-0.29148843030554333</span><span class="w">
	</span><span class="p">},</span><span class="w">
	</span><span class="nl">"action"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
	</span><span class="nl">"reward"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w">
	</span><span class="nl">"next_state_features"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nl">"0"</span><span class="p">:</span><span class="w"> </span><span class="mf">-0.02587951488077904</span><span class="p">,</span><span class="w">
		</span><span class="nl">"1"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0019959027899765502</span><span class="p">,</span><span class="w">
		</span><span class="nl">"2"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.00723572196842974</span><span class="p">,</span><span class="w">
		</span><span class="nl">"3"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.005286388581067669</span><span class="w">
	</span><span class="p">},</span><span class="w">
	</span><span class="nl">"time_diff"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
	</span><span class="nl">"possible_next_actions"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">],</span><span class="w">
	</span><span class="nl">"metrics"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nl">"reward"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="w">
	</span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<blockquote>
  <p>Interesting here is that the Spark engine will allow us to utilize a distributed cluster running completely with CPU operations. The GPU operations will come in a later stage. Allowing us to completely utilize one cluster on HDFS and one cluster purely for the GPU calculations.</p>
</blockquote>

<h3 id="3-normalization">3. Normalization</h3>

<p>To reduce noise and train our Neural Networks faster, we utilize “Normalization”. Horizon includes a tool that automatically analyzes the training dataset and determines the best transformation function and corresponding normalization parameters for each feature.</p>

<p>To run this, the following command can be used:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">python</span> <span class="n">ml</span><span class="o">/</span><span class="n">rl</span><span class="o">/</span><span class="n">workflow</span><span class="o">/</span><span class="n">create_normalization_metadata</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">p</span> <span class="n">ml</span><span class="o">/</span><span class="n">rl</span><span class="o">/</span><span class="n">workflow</span><span class="o">/</span><span class="n">sample_configs</span><span class="o">/</span><span class="n">discrete_action</span><span class="o">/</span><span class="n">dqn_example</span><span class="o">.</span><span class="n">json</span>
</code></pre></div></div>

<p>Opening up the <code class="highlighter-rouge">/ml/rl/workflow/sample_configs/discrete_action/dqn_example.json</code> file we can see a similar config file as passed to the main function of our Gym Environment:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
	</span><span class="nl">"training_data_path"</span><span class="p">:</span><span class="w"> </span><span class="s2">"training_data/cartpole_training_data.json"</span><span class="p">,</span><span class="w">
	</span><span class="nl">"state_norm_data_path"</span><span class="p">:</span><span class="w"> </span><span class="s2">"training_data/state_features_norm.json"</span><span class="p">,</span><span class="w">
	</span><span class="nl">"model_output_path"</span><span class="p">:</span><span class="w"> </span><span class="s2">"outputs/"</span><span class="p">,</span><span class="w">
	</span><span class="nl">"use_gpu"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
	</span><span class="nl">"use_all_avail_gpus"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
	</span><span class="nl">"norm_params"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nl">"output_dir"</span><span class="p">:</span><span class="w"> </span><span class="s2">"training_data/"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"cols_to_norm"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
			</span><span class="s2">"state_features"</span><span class="w">
		</span><span class="p">],</span><span class="w">
		</span><span class="nl">"num_samples"</span><span class="p">:</span><span class="w"> </span><span class="mi">1000</span><span class="w">
	</span><span class="p">},</span><span class="w">
	</span><span class="nl">"actions"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
		</span><span class="s2">"0"</span><span class="p">,</span><span class="w">
		</span><span class="s2">"1"</span><span class="w">
	</span><span class="p">],</span><span class="w">
	</span><span class="nl">"epochs"</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span><span class="p">,</span><span class="w">
	</span><span class="nl">"rl"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nl">"gamma"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.99</span><span class="p">,</span><span class="w">
		</span><span class="nl">"target_update_rate"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.2</span><span class="p">,</span><span class="w">
		</span><span class="nl">"reward_burnin"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
		</span><span class="nl">"maxq_learning"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w">
		</span><span class="nl">"epsilon"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.2</span><span class="p">,</span><span class="w">
		</span><span class="nl">"temperature"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.35</span><span class="p">,</span><span class="w">
		</span><span class="nl">"softmax_policy"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="w">
	</span><span class="p">},</span><span class="w">
	</span><span class="nl">"rainbow"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nl">"double_q_learning"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w">
		</span><span class="nl">"dueling_architecture"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
	</span><span class="p">},</span><span class="w">
	</span><span class="nl">"training"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
		</span><span class="nl">"layers"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">-1</span><span class="p">,</span><span class="w">
			</span><span class="mi">128</span><span class="p">,</span><span class="w">
			</span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">-1</span><span class="w">
		</span><span class="p">],</span><span class="w">
		</span><span class="nl">"activations"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
			</span><span class="s2">"relu"</span><span class="p">,</span><span class="w">
			</span><span class="s2">"relu"</span><span class="p">,</span><span class="w">
			</span><span class="s2">"linear"</span><span class="w">
		</span><span class="p">],</span><span class="w">
		</span><span class="nl">"minibatch_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w">
		</span><span class="nl">"learning_rate"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.001</span><span class="p">,</span><span class="w">
		</span><span class="nl">"optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ADAM"</span><span class="p">,</span><span class="w">
		</span><span class="nl">"lr_decay"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.999</span><span class="p">,</span><span class="w">
		</span><span class="nl">"warm_start_model_path"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
		</span><span class="nl">"l2_decay"</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w">
		</span><span class="nl">"use_noisy_linear_layers"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
	</span><span class="p">},</span><span class="w">
	</span><span class="nl">"in_training_cpe"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>So let’s open up the <code class="highlighter-rouge">ml/rl/workflow/create_normalization_metadata.py</code> file where we can instantly see in its main method that it starts with a function called: <code class="highlighter-rouge">create_norm_table</code>.</p>

<p>The <code class="highlighter-rouge">create_norm_table()</code> method will take in the parameters (which is the json above) and utilize the <code class="highlighter-rouge">norm_params</code>, <code class="highlighter-rouge">training_data_path</code>, <code class="highlighter-rouge">cols_to_norm</code> and <code class="highlighter-rouge">output_dir</code> configs to create the normalization table.</p>

<p>This normalization table is build by checking the columns to normalize (which in the case of the json above is the column <code class="highlighter-rouge">state_features</code>) which will get the metadata through the <code class="highlighter-rouge">get_norm_metadata()</code> function. This function will start reading from our dataset and start sampling the features and its values. Once it collected enough samples (as defined by the <code class="highlighter-rouge">norm_params["num_samples]</code> configuration), it will continue.</p>

<h3 id="4-training-the-model">4. Training the Model</h3>

<p>Since everything is pre-processed now and the data is normalized, we are ready to start training our model. For this we can run the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python ml/rl/workflow/dqn_workflow.py <span class="nt">-p</span> ml/rl/workflow/sample_configs/discrete_action/dqn_example.json
</code></pre></div></div>

<p>Which will utilize the <a href="https://github.com/facebookresearch/Horizon/blob/master/ml/rl/workflow/dqn_workflow.py">dqn_workflow.py</a> file that will choose the correct trainer to train its model. Running this will result in:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INFO:__main__:CPE evaluation took 0.23067665100097656 seconds.
INFO:__main__:Training finished. Processed ~3961 examples / s.
INFO:ml.rl.workflow.helpers:Saving PyTorch trainer to outputs/trainer_1543773299.pt
INFO:ml.rl.workflow.helpers:Saving Caffe2 predictor to outputs/predictor_1543773299.c2
INFO:ml.rl.caffe_utils:INPUT BLOB: input.1. OUTPUT BLOB:11
INFO:ml.rl.training.dqn_predictor:Generated ONNX predict net:
INFO:ml.rl.training.dqn_predictor:name: <span class="s2">"torch-jit-export_predict"</span>
op <span class="o">{</span>
  input: <span class="s2">"input.1"</span>
  input: <span class="s2">"1"</span>
  input: <span class="s2">"2"</span>
  output: <span class="s2">"7"</span>
  name: <span class="s2">""</span>
  <span class="nb">type</span>: <span class="s2">"FC"</span>
<span class="o">}</span>
op <span class="o">{</span>
  input: <span class="s2">"7"</span>
  output: <span class="s2">"8"</span>
  name: <span class="s2">""</span>
  <span class="nb">type</span>: <span class="s2">"Relu"</span>
<span class="o">}</span>
op <span class="o">{</span>
  input: <span class="s2">"8"</span>
  input: <span class="s2">"3"</span>
  input: <span class="s2">"4"</span>
  output: <span class="s2">"9"</span>
  name: <span class="s2">""</span>
  <span class="nb">type</span>: <span class="s2">"FC"</span>
<span class="o">}</span>
op <span class="o">{</span>
  input: <span class="s2">"9"</span>
  output: <span class="s2">"10"</span>
  name: <span class="s2">""</span>
  <span class="nb">type</span>: <span class="s2">"Relu"</span>
<span class="o">}</span>
op <span class="o">{</span>
  input: <span class="s2">"10"</span>
  input: <span class="s2">"5"</span>
  input: <span class="s2">"6"</span>
  output: <span class="s2">"11"</span>
  name: <span class="s2">""</span>
  <span class="nb">type</span>: <span class="s2">"FC"</span>
<span class="o">}</span>
device_option <span class="o">{</span>
  device_type: 0
  device_id: 0
<span class="o">}</span>
external_input: <span class="s2">"input.1"</span>
external_input: <span class="s2">"1"</span>
external_input: <span class="s2">"2"</span>
external_input: <span class="s2">"3"</span>
external_input: <span class="s2">"4"</span>
external_input: <span class="s2">"5"</span>
external_input: <span class="s2">"6"</span>
external_output: <span class="s2">"11"</span>

INFO:ml.rl.preprocessing.preprocessor_net:Processed <span class="nb">split</span> <span class="o">(</span>0, 4<span class="o">)</span> <span class="k">for </span>feature <span class="nb">type </span>CONTINUOUS
INFO:ml.rl.preprocessing.preprocessor_net:input# 0: preprocessor_net.py:287:Where_output0
</code></pre></div></div>

<h3 id="5-evaluating-the-model">5. Evaluating the Model</h3>

<p>The model is trained, but how do we test it? This can be done through the included eval script for the cartpole experiment:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python ml/rl/test/workflow/eval_cartpole.py <span class="nt">-m</span> outputs/predictor_&lt;number&gt;.c2
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">model_path</span><span class="p">):</span>
    <span class="n">predictor</span> <span class="o">=</span> <span class="n">DQNPredictor</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="s">"minidb"</span><span class="p">,</span> <span class="n">int_features</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="n">env</span> <span class="o">=</span> <span class="n">OpenAIGymEnvironment</span><span class="p">(</span><span class="n">gymenv</span><span class="o">=</span><span class="n">ENV</span><span class="p">)</span>

    <span class="n">avg_rewards</span><span class="p">,</span> <span class="n">avg_discounted_rewards</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">run_ep_n_times</span><span class="p">(</span>
        <span class="n">AVG_OVER_NUM_EPS</span><span class="p">,</span> <span class="n">predictor</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="bp">True</span>
    <span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="s">"Achieved an average reward score of {} over {} evaluations."</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span>
            <span class="n">avg_rewards</span><span class="p">,</span> <span class="n">AVG_OVER_NUM_EPS</span>
        <span class="p">)</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">parse_args</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span><span class="s">"Usage: python &lt;file.py&gt; -m &lt;parameters_file&gt;"</span><span class="p">)</span>

    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s">"Read command line parameters."</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">"-m"</span><span class="p">,</span> <span class="s">"--model"</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"Path to Caffe2 model."</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="k">return</span> <span class="n">args</span><span class="o">.</span><span class="n">model</span>
</code></pre></div></div>

<p>This will run our Gym Environment with the given model and return the reward score over x evaluation as a log line.</p>

<p>An example of this is:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INFO:__main__:Achieved an average reward score of 9.34 over 100 evaluations.
</code></pre></div></div>

<h3 id="6-visualizing-via-tensorboard">6. Visualizing via Tensorboard</h3>

<p>Now the last step is to visualize everything through Tensorboard, which we can start as:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensorboard <span class="nt">--logdir</span> outputs/
</code></pre></div></div>

<p>Which will spawn a process that binds to <a href="https://localhost:6006">https://localhost:6006</a>.</p>

<h2 id="rendering-our-trained-model">Rendering our Trained Model</h2>

<p>Once we trained our model, we were able to visualize it through Tensorboard. However we also would like to be able to view the model running while evaluating it. To be able to start with this, first install the prerequisites + dependencies as shown here: <a href="/running-openai-gym-on-windows-and-js">How to run OpenAI Gym on Windows and with Javascript</a>.</p>

<p>Another thing we now have to do is to change the <code class="highlighter-rouge">eval_cartpole.py</code> file and add <code class="highlighter-rouge">render=True</code> in the <code class="highlighter-rouge">run_ep_n_times()</code> method, making it look like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">avg_rewards</span><span class="p">,</span> <span class="n">avg_discounted_rewards</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">run_ep_n_times</span><span class="p">(</span>
    <span class="n">AVG_OVER_NUM_EPS</span><span class="p">,</span> <span class="n">predictor</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">render</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
</code></pre></div></div>

<p>When we now relaunch our evaluator through:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python ml/rl/test/workflow/eval_cartpole.py <span class="nt">-m</span> outputs/predictor_&lt;number&gt;.c2
</code></pre></div></div>

<p>We will be able to see our process spawn:</p>

<p><img src="/assets/images/posts/fb-horizon/cartpole.png" alt="/assets/images/posts/fb-horizon/cartpole.png" /></p>

                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
            

            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                        <section class="author-card">
                            
                                <img class="author-profile-image" src="/assets/images/avatar.jpg" alt="xavier" />
                            
                            <section class="author-card-content">
                                <h4 class="author-card-name"><a href="/author/xavier">Xavier Geerinck</a></h4>
                                
                                    <p>Xavier works as a Cloud Solution Architect at Microsoft, helping its customer unlock the full potential of the cloud. Even though he is still considered a young graduate, he achieved his first success at the age 16, by creating and selling his first startup. He then took this knowledge to create and help more startups in different markets such as technology, social media, philanthropy and home care. While in the meantime gaining more enterprise insights at renowned enterprises such as Nokia, Cisco and now Microsoft.</p>
                                
                            </section>
                        </section>
                        <div class="post-full-footer-right">
                            <a class="author-card-button" href="/author/xavier">Read More</a>
                        </div>
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            this.page.url = window.location.href;
                            this.page.identifier = '/facebook-horizon';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://xaviergeerinck.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            

        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/images/covers/blog-cover.png)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; Xavier Geerinck - Blog &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/ai/">Ai</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/ai-battlecards">AI Battlecards - End to End Process for building and evaluating AI models</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/ai-performance-markers">Artificial Intelligence - How to measure performance - Accuracy, Precision, Recall, F1, ROC, RMSE, F-Test and R-Squared</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/web2text">Web2Text - Deep Structured Boilerplate Removal - Running the Code</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/ai/">
                                
                                    See all 13 posts  →
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                
    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/rl-overview-terminology">
                <div class="post-card-image" style="background-image: url(/assets/images/covers/rl.jpg)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/rl-overview-terminology">
                <header class="post-card-header">
                    
                        
                            
                               <span class="post-card-tags">Ai</span>
                            
                        
                            
                               <span class="post-card-tags">Ai-ml</span>
                            
                        
                            
                                <span class="post-card-tags">Ai-rl</span>
                            
                        
                    

                    <h2 class="post-card-title">Reinforcement Learning - Terminology</h2>
                </header>
                <section class="post-card-excerpt">
                    <p>Tags: Machine LearningReinforcement LearningQ LearningArtificial IntelligenceMonte Carlo Reinforcement Learning - An Overview of Today Reinforcement Learning took big leaps recently, so big that they are being used more and more in production environments</p>
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                        
                        <img class="author-profile-image" src="/assets/images/avatar.jpg" alt="Xavier Geerinck" />
                        
                        <span class="post-card-author">
                            <a href="/author/xavier/">Xavier Geerinck</a>
                        </span>
                    
                
            </footer>
        </div>
    </article>

            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                
    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/the-achievers-mind">
                <div class="post-card-image" style="background-image: url(/assets/images/covers/experience6.jpg)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/the-achievers-mind">
                <header class="post-card-header">
                    
                        
                            
                               <span class="post-card-tags">Experience</span>
                            
                        
                            
                                <span class="post-card-tags">Business</span>
                            
                        
                    

                    <h2 class="post-card-title">The Achievers Mindset</h2>
                </header>
                <section class="post-card-excerpt">
                    <p>When I tell people that I had 5 startups (yes, see [my projects](/projects)), worked for 2, graduated 1.5 years ago as a Industrial Engineer in Computer Science and straight out of school got</p>
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                        
                        <img class="author-profile-image" src="/assets/images/avatar.jpg" alt="Xavier Geerinck" />
                        
                        <span class="post-card-author">
                            <a href="/author/xavier/">Xavier Geerinck</a>
                        </span>
                    
                
            </footer>
        </div>
    </article>

            
        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="/">
            
                <img src="/assets/images/favicon.png" alt="Xavier Geerinck - Blog icon" />
            
            <span>Xavier Geerinck - Blog</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">Facebook's Open-Source Reinforcement Learning Platform - A Deep Dive</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=Facebook%27s+Open-Source+Reinforcement+Learning+Platform+-+A+Deep+Dive&amp;url=https://thebillkidy.github.io/facebook-horizon"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://thebillkidy.github.io/facebook-horizon"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
        <a class="floating-header-share-li" href="https://www.linkedin.com/shareArticle?mini=true&url=https://thebillkidy.github.io/facebook-horizon&title=Facebook%27s+Open-Source+Reinforcement+Learning+Platform+-+A+Deep+Dive&source=https://thebillkidy.github.io/"
            onclick="window.open(this.href, 'share-linkedin','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg>
        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Xavier Geerinck - Blog</a> &copy; 2020</section>
                <section class="poweredby">Made with ❤ in Belgium</section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    <a href="https://twitter.com/XavierGeerinck" target="_blank" rel="noopener">Twitter</a>
                    <a href="https://linkedin.com/in/xaviergeerinck" target="_blank" rel="noopener">LinkedIn</a>
                </nav>
            </div>
        </footer>
    </div>

    <!-- The big email subscribe modal content -->
     

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
    <!-- <script>hljs.initHighlightingOnLoad();</script> -->

    <script>
        var blocks = document.querySelectorAll('div[class^=language-]');
        blocks.forEach((b) => {
            var language = b.className.split(" ")[0].match(/language\-(.*)/)[1];

            var codeBlocks = b.querySelectorAll('pre code');

            // Highlight the code
            codeBlocks.forEach((i) => {
                var text = i.innerText;
                var highlightedText = hljs.highlight(language, text);
                i.innerHTML = highlightedText.value;
            });

            // Add line numbers

            codeBlocks.forEach((i) => {            
                i.innerHTML = i.innerHTML.split("\n").map(function (i, idx, arr) {
                    // The last line seems to be empty, so don't work with it
                    if (idx == arr.length - 1) {
                        return '';
                    }

                    return '<span class="line-numbers"">' + i + '</span>' + "\n";
                }).join("");
            });
        });
    </script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.3.1.min.js"
        integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
        crossorigin="anonymous"></script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/fitvids/1.2.0/jquery.fitvids.min.js"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-27398198-1', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

</body>
</html>
